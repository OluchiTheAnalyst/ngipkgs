<?xml version="1.0" encoding="utf-8"?>
<!-- name="GENERATOR" content="github.com/mmarkdown/mmark Mmark Markdown Processor - mmark.miek.nl" -->
<rfc version="3" ipr="trust200902" docName="draft-XRFRAGMENTS-leonvankammen-00" submissionType="IETF" category="info" xml:lang="en" xmlns:xi="http://www.w3.org/2001/XInclude" indexInclude="true" consensus="true">

<front>
<title>XR Fragments</title><seriesInfo value="draft-XRFRAGMENTS-leonvankammen-00" stream="IETF" status="informational" name="XR-Fragments"></seriesInfo>
<author initials="L.R." surname="van Kammen" fullname="L.R. van Kammen"><organization></organization><address><postal><street></street>
</postal></address></author><date/>
<area>Internet</area>
<workgroup>Internet Engineering Task Force</workgroup>

<abstract>
<t>This draft offers a specification for 4D URLs &amp; navigation, to link 3D scenes and text together with- or without a network-connection.<br />

The specification promotes spatial addressibility, sharing, navigation, query-ing and tagging interactive (text)objects across for (XR) Browsers.<br />

XR Fragments allows us to enrich existing dataformats, by recursive use of existing proven technologies like <eref target="https://en.wikipedia.org/wiki/URI_fragment">URI Fragments</eref> and <eref target="https://visual-meta.info">visual-meta</eref>.<br />
</t>
</abstract>

</front>

<middle>

<section anchor="introduction"><name>Introduction</name>
<t>How can we add more features to existing text &amp; 3D scenes, without introducing new dataformats?<br />

Historically, there's many attempts to create the ultimate markuplanguage or 3D fileformat.<br />

However, thru the lens of authoring their lowest common denominator is still: plain text.<br />

XR Fragments allows us to enrich existing dataformats, by recursive use of existing technologies:<br />
</t>

<ol spacing="compact">
<li>addressibility and navigation of 3D scenes/objects: <eref target="https://en.wikipedia.org/wiki/URI_fragment">URI Fragments</eref> + src/href spatial metadata</li>
<li>hasslefree tagging across text and spatial objects using BiBTeX (<eref target="https://visual-meta.info">visual-meta</eref> e.g.)</li>
</ol>
<blockquote><t>NOTE: The chapters in this document are ordered from highlevel to lowlevel (technical) as much as possible</t>
</blockquote></section>

<section anchor="conventions-and-definitions"><name>Conventions and Definitions</name>
<table>
<thead>
<tr>
<th>definition</th>
<th>explanation</th>
</tr>
</thead>

<tbody>
<tr>
<td>human</td>
<td>a sentient being who thinks fuzzy, absorbs, and shares thought (by plain text, not markuplanguage)</td>
</tr>

<tr>
<td>scene</td>
<td>a (local/remote) 3D scene or 3D file (index.gltf e.g.)</td>
</tr>

<tr>
<td>3D object</td>
<td>an object inside a scene characterized by vertex-, face- and customproperty data.</td>
</tr>

<tr>
<td>metadata</td>
<td>custom properties of text, 3D Scene or Object(nodes), relevant to machines and a human minority (academics/developers)</td>
</tr>

<tr>
<td>XR fragment</td>
<td>URI Fragment with spatial hints (<tt>#pos=0,0,0&amp;t=1,100</tt> e.g.)</td>
</tr>

<tr>
<td>src</td>
<td>(HTML-piggybacked) metadata of a 3D object which instances content</td>
</tr>

<tr>
<td>href</td>
<td>(HTML-piggybacked) metadata of a 3D object which links to content</td>
</tr>

<tr>
<td>query</td>
<td>an URI Fragment-operator which queries object(s) from a scene (<tt>#q=cube</tt>)</td>
</tr>

<tr>
<td>visual-meta</td>
<td><eref target="https://visual.meta.info">visual-meta</eref> data appended to text which is indirectly visible/editable in XR.</td>
</tr>

<tr>
<td>requestless metadata</td>
<td>opposite of networked metadata (RDF/HTML request-fanouts easily cause framerate-dropping, hence not used a lot in games).</td>
</tr>

<tr>
<td>FPS</td>
<td>frames per second in spatial experiences (games,VR,AR e.g.), should be as high as possible</td>
</tr>

<tr>
<td>introspective</td>
<td>inward sensemaking (&quot;I feel this belongs to that&quot;)</td>
</tr>

<tr>
<td>extrospective</td>
<td>outward sensemaking (&quot;I'm fairly sure John is a person who lives in oklahoma&quot;)</td>
</tr>

<tr>
<td><tt>◻</tt></td>
<td>ascii representation of an 3D object/mesh</td>
</tr>
</tbody>
</table></section>

<section anchor="core-principle"><name>Core principle</name>
<t>XR Fragments strives to serve humans first, machine(implementations) later, by ensuring hasslefree text-to-thought feedback loops.<br />

This also means that the repair-ability of machine-matters should be human friendly too (not too complex).<br />
</t>
<blockquote><t>&quot;When a car breaks down, the ones without turbosupercharger are easier to fix&quot;</t>
</blockquote></section>

<section anchor="list-of-uri-fragments"><name>List of URI Fragments</name>
<table>
<thead>
<tr>
<th>fragment</th>
<th>type</th>
<th>example</th>
<th>info</th>
</tr>
</thead>

<tbody>
<tr>
<td><tt>#pos</tt></td>
<td>vector3</td>
<td><tt>#pos=0.5,0,0</tt></td>
<td>positions camera to xyz-coord 0.5,0,0</td>
</tr>

<tr>
<td><tt>#rot</tt></td>
<td>vector3</td>
<td><tt>#rot=0,90,0</tt></td>
<td>rotates camera to xyz-coord 0.5,0,0</td>
</tr>

<tr>
<td><tt>#t</tt></td>
<td>vector2</td>
<td><tt>#t=500,1000</tt></td>
<td>sets animation-loop range between frame 500 and 1000</td>
</tr>

<tr>
<td><tt>#......</tt></td>
<td>string</td>
<td><tt>#.cubes</tt> <tt>#cube</tt></td>
<td>object(s) of interest (fragment to object name or class mapping)</td>
</tr>
</tbody>
</table><blockquote><t>xyz coordinates are similar to ones found in SVG Media Fragments</t>
</blockquote></section>

<section anchor="list-of-metadata-for-3d-nodes"><name>List of metadata for 3D nodes</name>
<table>
<thead>
<tr>
<th>key</th>
<th>type</th>
<th>example (JSON)</th>
<th>info</th>
</tr>
</thead>

<tbody>
<tr>
<td><tt>name</tt></td>
<td>string</td>
<td><tt>&quot;name&quot;: &quot;cube&quot;</tt></td>
<td>available in all 3D fileformats &amp; scenes</td>
</tr>

<tr>
<td><tt>class</tt></td>
<td>string</td>
<td><tt>&quot;class&quot;: &quot;cubes&quot;</tt></td>
<td>available through custom property in 3D fileformats</td>
</tr>

<tr>
<td><tt>href</tt></td>
<td>string</td>
<td><tt>&quot;href&quot;: &quot;b.gltf&quot;</tt></td>
<td>available through custom property in 3D fileformats</td>
</tr>

<tr>
<td><tt>src</tt></td>
<td>string</td>
<td><tt>&quot;src&quot;: &quot;#q=cube&quot;</tt></td>
<td>available through custom property in 3D fileformats</td>
</tr>
</tbody>
</table><t>Popular compatible 3D fileformats: <tt>.gltf</tt>, <tt>.obj</tt>, <tt>.fbx</tt>, <tt>.usdz</tt>, <tt>.json</tt> (THREEjs), <tt>COLLADA</tt> and so on.</t>
<blockquote><t>NOTE: XR Fragments are file-agnostic, which means that the metadata exist in programmatic 3D scene(nodes) too.</t>
</blockquote></section>

<section anchor="navigating-3d"><name>Navigating 3D</name>
<t>Here's an ascii representation of a 3D scene-graph which contains 3D objects <tt>◻</tt> and their metadata:</t>

<artwork>  +--------------------------------------------------------+ 
  |                                                        |
  |  index.gltf                                            |
  |    │                                                   |
  |    ├── ◻ buttonA                                       |
  |    │      └ href: #pos=1,0,1&amp;t=100,200                 |
  |    │                                                   |
  |    └── ◻ buttonB                                       |
  |           └ href: other.fbx                            |   &lt;-- file-agnostic (can be .gltf .obj etc)
  |                                                        |
  +--------------------------------------------------------+

</artwork>
<t>An XR Fragment-compatible browser viewing this scene, allows the end-user to interact with the <tt>buttonA</tt> and <tt>buttonB</tt>.<br />

In case of <tt>buttonA</tt> the end-user will be teleported to another location and time in the <strong>current loaded scene</strong>, but <tt>buttonB</tt> will
 <strong>replace the current scene</strong> with a new one (<tt>other.fbx</tt>).</t>
</section>

<section anchor="embedding-3d-content"><name>Embedding 3D content</name>
<t>Here's an ascii representation of a 3D scene-graph with 3D objects (<tt>◻</tt>) which embeds remote &amp; local 3D objects (<tt>◻</tt>) (without) using queries:</t>

<artwork>  +--------------------------------------------------------+  +-------------------------+ 
  |                                                        |  |                         |
  |  index.gltf                                            |  | ocean.com/aquarium.fbx  |
  |    │                                                   |  |   │                     |
  |    ├── ◻ canvas                                        |  |   └── ◻ fishbowl        |
  |    │      └ src: painting.png                          |  |         ├─ ◻ bass       |
  |    │                                                   |  |         └─ ◻ tuna       |
  |    ├── ◻ aquariumcube                                  |  |                         |       
  |    │      └ src: ://rescue.com/fish.gltf#q=bass%20tuna |  +-------------------------+
  |    │                                                   |    
  |    ├── ◻ bedroom                                       |   
  |    │      └ src: #q=canvas                             |
  |    │                                                   |   
  |    └── ◻ livingroom                                    |      
  |           └ src: #q=canvas                             |
  |                                                        |
  +--------------------------------------------------------+
</artwork>
<t>An XR Fragment-compatible browser viewing this scene, lazy-loads and projects <tt>painting.png</tt> onto the (plane) object called <tt>canvas</tt> (which is copy-instanced in the bed and livingroom).<br />

Also, after lazy-loading <tt>ocean.com/aquarium.gltf</tt>, only the queried objects <tt>bass</tt> and <tt>tuna</tt> will be instanced inside <tt>aquariumcube</tt>.<br />

Resizing will be happen accordingly to its placeholder object (<tt>aquariumcube</tt>), see chapter Scaling.<br />
</t>
</section>

<section anchor="text-in-xr-tagging-linking-to-spatial-objects"><name>Text in XR (tagging,linking to spatial objects)</name>
<t>We still think and speak in simple text, not in HTML or RDF.<br />

It would be funny when people would shout <tt>&lt;h1&gt;FIRE!&lt;/h1&gt;</tt> in case of emergency.<br />

Given the myriad of new (non-keyboard) XR interfaces, keeping text as is (not obscuring with markup) is preferred.<br />

Ideally metadata must come <strong>later with</strong> text, but not <strong>obfuscate</strong> the text, or <strong>in another</strong> file.<br />
</t>
<blockquote><t>Humans first, machines (AI) later.</t>
</blockquote><t>This way:</t>

<ol spacing="compact">
<li>XR Fragments allows &lt;b id=&quot;tagging-text&quot;&gt;hasslefree XR text tagging&lt;/b&gt;, using BibTeX metadata <strong>at the end of content</strong> (like <eref target="https://visual.meta.info">visual-meta</eref>).</li>
<li>XR Fragments allows hasslefree &lt;a href=&quot;#textual-tag&quot;&gt;textual tagging&lt;/a&gt;, &lt;a href=&quot;#spatial-tag&quot;&gt;spatial tagging&lt;/a&gt;, and &lt;a href=&quot;#supra-tagging&quot;&gt;supra tagging&lt;/a&gt;, by mapping 3D/text object (class)names to BibTeX</li>
<li>inline BibTeX is the minimum required <strong>requestless metadata</strong>-layer for XR text, RDF/JSON is great but optional (and too verbose for the spec-usecases).</li>
<li>Default font (unless specified otherwise) is a modern monospace font, for maximized tabular expressiveness (see <eref target="#core-principle">the core principle</eref>).</li>
<li>anti-pattern: hardcoupling a mandatory <strong>obtrusive markuplanguage</strong> or framework with an XR browsers (HTML/VRML/Javascript) (see <eref target="#core-principle">the core principle</eref>)</li>
<li>anti-pattern: limiting human introspection, by immediately funneling human thought into typesafe, precise, pre-categorized metadata like RDF (see <eref target="#core-principle">the core principle</eref>)</li>
</ol>
<t>This allows recursive connections between text itself, as well as 3D objects and vice versa, using <strong>BiBTeX-tags</strong> :</t>

<artwork>  +--------------------------------------------------+
  | My Notes                                         |
  |                                                  |
  | The houses seen here are built in baroque style. |   
  |                                                  |   
  | @house{houses,                                &lt;----- XR Fragment triple/tag: tiny &amp; phrase-matching BiBTeX
  |   url  = {#.house}              &lt;------------------- XR Fragment URI
  | }                                                |
  +--------------------------------------------------+
</artwork>
<t>This sets up the following associations in the scene:</t>

<ol spacing="compact">
<li>&lt;b id=&quot;textual-tagging&quot;&gt;textual tag&lt;/b&gt;: text or spatial-occurences named 'houses' is now automatically tagged with 'house'</li>
<li>&lt;b id=&quot;spatial-tagging&quot;&gt;spatial tag&lt;/b&gt;: spatial object(s) with class:house (#.house) is now automatically tagged with 'house'</li>
<li>&lt;b id=&quot;supra-tagging&quot;&gt;supra-tag&lt;/b&gt;: text- or spatial-object named 'house' (spatially) elsewhere, is now automatically tagged with 'house'</li>
</ol>
<t>Spatial wires can be rendered, words can be highlighted, spatial objects can be highlighted, links can be manipulated by the user.</t>
<blockquote><t>The simplicity of appending BibTeX (humans first, machines later) is demonstrated by <eref target="https://visual-meta.info">visual-meta</eref> in greater detail, and makes it perfect for GUI's to generate (bib)text later. Humans can still view/edit the metadata manually, by clicking 'toggle metadata' on the 'back' (contextmenu e.g.) of any XR text, anywhere anytime.</t>
</blockquote>
<section anchor="default-data-uri-mimetype"><name>Default Data URI mimetype</name>
<t>The <tt>src</tt>-values work as expected (respecting mime-types), however:</t>
<t>The XR Fragment specification bumps the traditional default browser-mimetype</t>
<t><tt>text/plain;charset=US-ASCII</tt></t>
<t>to a green eco-friendly:</t>
<t><tt>text/plain;charset=utf-8;bibtex=^@</tt></t>
<t>This indicates that any bibtex metadata starting with <tt>@</tt> will automatically get filtered out and:</t>

<ul spacing="compact">
<li>automatically detects textual links between textual and spatial objects</li>
</ul>
<t>It's concept is similar to literate programming.
Its implications are that local/remote responses can now:</t>

<ul spacing="compact">
<li>(de)multiplex/repair human text and requestless metadata (see <eref target="#core-principle">the core principle</eref>)</li>
<li>no separated implementation/network-overhead for metadata (see <eref target="#core-principle">the core principle</eref>)</li>
<li>ensuring high FPS: HTML/RDF historically is too 'requesty' for game studios</li>
<li>rich send/receive/copy-paste everywhere by default, metadata being retained (see <eref target="#core-principle">the core principle</eref>)</li>
<li>less network requests, therefore less webservices, therefore less servers, and overall better FPS in XR</li>
</ul>
<blockquote><t>This significantly expands expressiveness and portability of human text, by <strong>postponing machine-concerns to the end of the human text</strong> in contrast to literal interweaving of content and markupsymbols (or extra network requests, webservices e.g.).</t>
</blockquote><t>For all other purposes, regular mimetypes can be used (but are not required by the spec).<br />

To keep XR Fragments a lightweight spec, BiBTeX is used for text-spatial object mappings (not a scripting language or RDF e.g.).</t>
<blockquote><t>Applications are also free to attach any JSON(LD / RDF) to spatial objects using custom properties (but is not interpreted by this spec).</t>
</blockquote></section>

<section anchor="url-and-data-uri"><name>URL and Data URI</name>

<artwork>  +--------------------------------------------------------------+  +------------------------+
  |                                                              |  | author.com/article.txt |
  |  index.gltf                                                  |  +------------------------+
  |    │                                                         |  |                        |
  |    ├── ◻ article_canvas                                      |  | Hello friends.         |
  |    │    └ src: ://author.com/article.txt                     |  |                        |
  |    │                                                         |  | @friend{friends        |
  |    └── ◻ note_canvas                                         |  |   ...                  |
  |           └ src:`data:welcome human @...`                    |  | }                      | 
  |                                                              |  +------------------------+
  |                                                              |
  +--------------------------------------------------------------+
</artwork>
<t>The enduser will only see <tt>welcome human</tt> and <tt>Hello friends</tt> rendered spatially.
The beauty is that text (AND visual-meta) in Data URI promotes rich copy-paste.
In both cases, the text gets rendered immediately (onto a plane geometry, hence the name '_canvas').
The XR Fragment-compatible browser can let the enduser access visual-meta(data)-fields after interacting with the object (contextmenu e.g.).</t>
<t>The mapping between 3D objects and text (src-data) is simple:</t>
<t>Example:</t>

<artwork>  +------------------------------------------------------------------------------------+ 
  |                                                                                    | 
  |  index.gltf                                                                        | 
  |    │                                                                               | 
  |    └── ◻ rentalhouse                                                               | 
  |           └ class: house                                                           | 
  |           └ ◻ note                                                                 | 
  |                 └ src:`data: todo: call owner                                      |
  |                              @house{owner,                                         |
  |                                url  = {#.house}                                    |
  |                              }`                                                    |
  +------------------------------------------------------------------------------------+
</artwork>
<t>Attaching visualmeta as <tt>src</tt> metadata to the (root) scene-node hints the XR Fragment browser.
3D object names and classes map to <tt>name</tt> of visual-meta glossary-entries.
This allows rich interaction and interlinking between text and 3D objects:</t>

<ol spacing="compact">
<li>When the user surfs to https://.../index.gltf#AI the XR Fragments-parser points the enduser to the AI object, and can show contextual info about it.</li>
<li>When (partial) remote content is embedded thru XR Fragment queries (see XR Fragment queries), its related visual-meta can be embedded along.</li>
</ol>
</section>

<section anchor="bibtex-as-lowest-common-denominator-for-tagging-triple"><name>BibTeX as lowest common denominator for tagging/triple</name>
<t>The everything-is-text focus of BiBTex is a great advantage for introspection, and perhaps a necessary bridge towards RDF (extrospective).
BibTeX-appendices (visual-meta e.g.) are already adopted in the physical world (academic books), perhaps due to its terseness &amp; simplicity:</t>

<ol spacing="compact">
<li>&lt;b id=&quot;frictionless-copy-paste&quot;&gt;frictionless copy/pasting&lt;/b&gt; (by humans) of (unobtrusive) content AND metadata</li>
<li>an introspective 'sketchpad' for metadata, which can (optionally) mature into RDF later</li>
</ol>
<table>
<thead>
<tr>
<th>characteristic</th>
<th>Plain Text (with BibTeX)</th>
<th>RDF</th>
</tr>
</thead>

<tbody>
<tr>
<td>perspective</td>
<td>introspective</td>
<td>extrospective</td>
</tr>

<tr>
<td>space/scope</td>
<td>local</td>
<td>world</td>
</tr>

<tr>
<td>everything is text (string)</td>
<td>yes</td>
<td>no</td>
</tr>

<tr>
<td>leaves (dictated) text intact</td>
<td>yes</td>
<td>no</td>
</tr>

<tr>
<td>markup language(s)</td>
<td>no (appendix)</td>
<td>~4 different</td>
</tr>

<tr>
<td>polyglot format</td>
<td>no</td>
<td>yes</td>
</tr>

<tr>
<td>easy to copy/paste content+metadata</td>
<td>yes</td>
<td>depends</td>
</tr>

<tr>
<td>easy to write/repair</td>
<td>yes</td>
<td>depends</td>
</tr>

<tr>
<td>easy to parse</td>
<td>yes (fits on A4 paper)</td>
<td>depends</td>
</tr>

<tr>
<td>infrastructure storage</td>
<td>selfcontained (plain text)</td>
<td>(semi)networked</td>
</tr>

<tr>
<td>tagging</td>
<td>yes</td>
<td>yes</td>
</tr>

<tr>
<td>freeform tagging/notes</td>
<td>yes</td>
<td>depends</td>
</tr>

<tr>
<td>specialized file-type</td>
<td>no</td>
<td>yes</td>
</tr>

<tr>
<td>copy-paste preserves metadata</td>
<td>yes</td>
<td>depends</td>
</tr>

<tr>
<td>emoji</td>
<td>yes</td>
<td>depends</td>
</tr>

<tr>
<td>predicates</td>
<td>free</td>
<td>pre-determined</td>
</tr>

<tr>
<td>implementation/network overhead</td>
<td>no</td>
<td>depends</td>
</tr>

<tr>
<td>used in (physical) books/PDF</td>
<td>yes (visual-meta)</td>
<td>no</td>
</tr>

<tr>
<td>terse categoryless predicates</td>
<td>yes</td>
<td>no</td>
</tr>

<tr>
<td>nested structures</td>
<td>no</td>
<td>yes</td>
</tr>
</tbody>
</table><blockquote><t>To serve humans first, human 'fuzzy symbolical mind' comes first, and <eref target="https://en.wikipedia.org/wiki/Borg">'categorized typesafe RDF hive mind'</eref>) later.</t>
</blockquote></section>

<section anchor="xr-text-bibtex-example-parser"><name>XR text (BibTeX) example parser</name>
<t>Here's a naive XR Text (de)multiplexer in javascript (which also supports visual-meta start/end-blocks):</t>

<artwork>xrtext = {
    
  decode: {
    text: (str) =&gt; {
        let meta={}, text='', last='', data = '';
        str.split(/\r?\n/).map( (line) =&gt; {
            if( !data ) data = last === '' &amp;&amp; line.match(/^@/) ? line[0] : ''  
            if( data ){
                if( line === '' ){
                    xrtext.decode.bibtex(data.substr(1),meta)
                    data=''
                }else data += `${line}\n`
            }
            text += data ? '' : `${line}\n`
            last=line
        })
        return {text, meta}      
    },
    bibtex: (str,meta) =&gt; {
        let st = [meta]
        str
        .split(/\r?\n/ )
        .map( s =&gt; s.trim() ).join(&quot;\n&quot;) // be nice
        .replace( /}@/,  &quot;}\n@&quot;  )       // to authors
        .replace( /},}/, &quot;},\n}&quot; )       // which struggle
        .replace( /^}/,  &quot;\n}&quot;   )       // with writing single-line BiBTeX
        .split(   /\n/           )       //
        .filter( c =&gt; c.trim()   )       // actual processing:
        .map( (s) =&gt; {
          if( s.match(/(^}|-end})/) &amp;&amp; st.length &gt; 1 ) st.shift()
          else if( s.match(/^@/)    ) st.unshift( st[0][ s.replace(/(-start|,)/g,'') ] = {} )
          else s.replace( /(\w+)\s*=\s*{(.*)}(,)?/g, (m,k,v) =&gt; st[0][k] = v )
        })
        return meta
    }
  },
    
  encode: (text,meta) =&gt; {
    if( text === false ){
        if (typeof meta === &quot;object&quot;) {
           return Object.keys(meta).map(k =&gt; 
               typeof meta[k] == &quot;string&quot; 
               ? `  ${k} = {${meta[k]}},`
               : `${ k.match(/[}{]$/) ? k.replace('}','-start}') : `${k},` }\n` +
                 `${ xrtext.encode( false, meta[k])}\n`                         +
                 `${  k.match(/}$/) ? k.replace('}','-end}') : '}' }\n`
                 .split(&quot;\n&quot;).filter( s =&gt; s.trim() ).join(&quot;\n&quot;)
            )
            .join(&quot;\n&quot;)
        }
        return meta.toString();
    }else return `${text}\n${xrtext.encode(false,meta)}`
  }

}

var {meta,text} = xrtext.decode.text(str)          // demultiplex text &amp; bibtex
meta['@foo{']   = { &quot;note&quot;:&quot;note from the user&quot;}   // edit metadata
xrtext.encode(text,meta)                           // multiplex text &amp; bibtex back together 
</artwork>
<blockquote><t>above can be used as a startingpoint for LLVM's to translate/steelman to any language.</t>
</blockquote></section>
</section>

<section anchor="hyper-copy-paste"><name>HYPER copy/paste</name>
<t>The previous example, offers something exciting compared to simple copy/paste of 3D objects or text.
XR Fragment allows HYPER-copy/paste: time, space and text interlinked.
Therefore, the enduser in an XR Fragment-compatible browser can copy/paste/share data in these ways:</t>

<ul spacing="compact">
<li>time/space: 3D object (current animation-loop)</li>
<li>text: TeXt object (including BiBTeX/visual-meta if any)</li>
<li>interlinked: Collected objects by visual-meta tag</li>
</ul>
</section>

<section anchor="xr-fragment-queries"><name>XR Fragment queries</name>
<t>Include, exclude, hide/shows objects using space-separated strings:</t>

<ul spacing="compact">
<li><tt>#q=cube</tt></li>
<li><tt>#q=cube -ball_inside_cube</tt></li>
<li><tt>#q=* -sky</tt></li>
<li><tt>#q=-.language .english</tt></li>
<li><tt>#q=cube&amp;rot=0,90,0</tt></li>
<li><tt>#q=price:&gt;2 price:&lt;5</tt></li>
</ul>
<t>It's simple but powerful syntax which allows &lt;b&gt;css&lt;/b&gt;-like class/id-selectors with a searchengine prompt-style feeling:</t>

<ol spacing="compact">
<li>queries are only executed when &lt;b&gt;embedded&lt;/b&gt; in the asset/scene (thru <tt>src</tt>). This is to prevent sharing of scene-tampered URL's.</li>
<li>search words are matched against 3D object names or metadata-key(values)</li>
<li><tt>#</tt> equals <tt>#q=*</tt></li>
<li>words starting with <tt>.</tt> (<tt>.language</tt>) indicate class-properties</li>
</ol>
<blockquote><t>*(*For example**: <tt>#q=.foo</tt> is a shorthand for <tt>#q=class:foo</tt>, which will select objects with custom property <tt>class</tt>:<tt>foo</tt>. Just a simple <tt>#q=cube</tt> will simply select an object named <tt>cube</tt>.</t>
</blockquote>
<ul spacing="compact">
<li>see <eref target="https://coderofsalvation.github.io/xrfragment.media/queries.mp4">an example video here</eref></li>
</ul>

<section anchor="including-excluding"><name>including/excluding</name>
<t>|''operator''  | ''info'' |
|<tt>*</tt> | select all objects (only allowed in <tt>src</tt> custom property) in the &lt;b&gt;current&lt;/b&gt; scene (&lt;b&gt;after&lt;/b&gt; the default [[predefined_view|predefined_view]] <tt>#</tt> was executed)|
|<tt>-</tt> | removes/hides object(s) |
|<tt>:</tt> | indicates an object-embedded custom property key/value |
|<tt>.</tt> | alias for <tt>class:</tt> (<tt>.foo</tt> equals <tt>class:foo</tt> |
|<tt>&gt;</tt> <tt>&lt;</tt>| compare float or int number|
|<tt>/</tt> | reference to root-scene.<br />
Useful in case of (preventing) showing/hiding objects in nested scenes (instanced by [[src]])<br />
<tt>#q=-/cube</tt> hides object <tt>cube</tt> only in the root-scene (not nested <tt>cube</tt> objects)<br />
 <tt>#q=-cube</tt> hides both object <tt>cube</tt> in the root-scene &lt;b&gt;AND&lt;/b&gt; nested <tt>skybox</tt> objects |</t>
<t><eref target="https://github.com/coderofsalvation/xrfragment/blob/main/src/3rd/js/three/xrf/q.js">» example implementation</eref>
<eref target="https://github.com/coderofsalvation/xrfragment/blob/main/example/assets/query.gltf#L192">» example 3D asset</eref>
<eref target="https://github.com/coderofsalvation/xrfragment/issues/3">» discussion</eref></t>
</section>

<section anchor="query-parser"><name>Query Parser</name>
<t>Here's how to write a query parser:</t>

<ol spacing="compact">
<li>create an associative array/object to store query-arguments as objects</li>
<li>detect object id's &amp; properties <tt>foo:1</tt> and <tt>foo</tt> (reference regex: <tt>/^.*:[&gt;&lt;=!]?/</tt>  )</li>
<li>detect excluders like <tt>-foo</tt>,<tt>-foo:1</tt>,<tt>-.foo</tt>,<tt>-/foo</tt> (reference regex: <tt>/^-/</tt> )</li>
<li>detect root selectors like <tt>/foo</tt> (reference regex: <tt>/^[-]?\//</tt> )</li>
<li>detect class selectors like <tt>.foo</tt> (reference regex: <tt>/^[-]?class$/</tt> )</li>
<li>detect number values like <tt>foo:1</tt> (reference regex: <tt>/^[0-9\.]+$/</tt> )</li>
<li>expand aliases like <tt>.foo</tt> into <tt>class:foo</tt></li>
<li>for every query token split string on <tt>:</tt></li>
<li>create an empty array <tt>rules</tt></li>
<li>then strip key-operator: convert &quot;-foo&quot; into &quot;foo&quot;</li>
<li>add operator and value to rule-array</li>
<li>therefore we we set <tt>id</tt> to <tt>true</tt> or <tt>false</tt> (false=excluder <tt>-</tt>)</li>
<li>and we set <tt>root</tt> to <tt>true</tt> or <tt>false</tt> (true=<tt>/</tt> root selector is present)</li>
<li>we convert key '/foo' into 'foo'</li>
<li>finally we add the key/value to the store (<tt>store.foo = {id:false,root:true}</tt> e.g.)</li>
</ol>
<blockquote><t>An example query-parser (which compiles to many languages) can be <eref target="https://github.com/coderofsalvation/xrfragment/blob/main/src/xrfragment/Query.hx">found here</eref></t>
</blockquote></section>

<section anchor="xr-fragment-uri-grammar"><name>XR Fragment URI Grammar</name>

<artwork>reserved    = gen-delims / sub-delims
gen-delims  = &quot;#&quot; / &quot;&amp;&quot;
sub-delims  = &quot;,&quot; / &quot;=&quot;
</artwork>
<blockquote><t>Example: <tt>://foo.com/my3d.gltf#pos=1,0,0&amp;prio=-5&amp;t=0,100</tt></t>
</blockquote><table>
<thead>
<tr>
<th>Demo</th>
<th>Explanation</th>
</tr>
</thead>

<tbody>
<tr>
<td><tt>pos=1,2,3</tt></td>
<td>vector/coordinate argument e.g.</td>
</tr>

<tr>
<td><tt>pos=1,2,3&amp;rot=0,90,0&amp;q=.foo</tt></td>
<td>combinators</td>
</tr>
</tbody>
</table></section>
</section>

<section anchor="security-considerations"><name>Security Considerations</name>
<t>Since XR Text contains metadata too, the user should be able to set up tagging-rules, so the copy-paste feature can :</t>

<ul spacing="compact">
<li>filter out sensitive data when copy/pasting (XR text with <tt>class:secret</tt> e.g.)</li>
</ul>
</section>

<section anchor="iana-considerations"><name>IANA Considerations</name>
<t>This document has no IANA actions.</t>
</section>

<section anchor="acknowledgments"><name>Acknowledgments</name>
<t>TODO acknowledge.</t>
</section>

</middle>

</rfc>
